{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import gdown\n",
    "import datetime as dt\n",
    "import sqlite3\n",
    "import kaggle\n",
    "import scipy.stats as sts\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pylab\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_dataset_download(kaggle_path, kaggle_path_Name, kaggle_zip_file):\n",
    "    kaggle.api.authenticate()\n",
    "    if kaggle_zip_file:\n",
    "        kaggle.api.dataset_download_files(kaggle_path, kaggle_path_Name, unzip=True)\n",
    "    else:\n",
    "        kaggle.api.dataset_download_files(kaggle_path, kaggle_path_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nulls_data(df):\n",
    "    #We want to know the quality of data. So, let's start by detecting not null percentage related to every column. \n",
    "    \n",
    "    df_tot_nulls = df.isnull().sum().sort_values(ascending=False)\n",
    "    df_tot_nulls_perc = 100 - round(df_tot_nulls/len(df)*100,2)\n",
    "    df_tot_perc_nulls = pd.concat([df_tot_nulls,df_tot_nulls_perc],axis=1)\n",
    "    df_tot_perc_nulls = df_tot_perc_nulls.rename(columns={0: \"Total\", 1: \"PercNotNull\"})\n",
    "    return df_tot_perc_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compound_acceptance_index(row):\n",
    "    if row['Sentiment'] == 'Extremely Positive':\n",
    "      return 1\n",
    "    if row['Sentiment'] == 'Positive':\n",
    "      return 0.5\n",
    "    if row['Sentiment'] == 'Neutral':\n",
    "      return 0\n",
    "    if row['Sentiment'] == 'Negative':\n",
    "      return -0.5\n",
    "    if row['Sentiment'] == 'Extremely Negative':\n",
    "      return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_hypothesis_determination(pval, p_alpha):\n",
    "    # Example: If alpha (significance) value is 0.05 or 5% it means 95% of confidence\n",
    "    confidence_perc = 1 - p_alpha\n",
    "    confidence_perc = 100 * confidence_perc\n",
    "    \n",
    "    p_alpha_perc = 100 * p_alpha\n",
    "    \n",
    "    str_H1 = \"I have enough evidence to reject H0. Therefore, I assume H1 with a confidence of {0}% and significance of {1}%\"\n",
    "    str_H0 = \"I don't have enough evidence to reject H0. So we accept is true with a confidence of {0}% and significance of {1}%\"\n",
    "    \n",
    "    if pval < p_alpha:\n",
    "       print(str_H1.format(confidence_perc,p_alpha_perc))\n",
    "    else:\n",
    "      print(str_H0.format(confidence_perc, p_alpha_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_geodf(df, lat_col_name='latitude', lon_col_name='longitude'):\n",
    "    df = df.copy()\n",
    "    lat = df[lat_col_name]\n",
    "    lon = df[lon_col_name]\n",
    "    return gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(lon, lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clinical_dementia_rating(row):\n",
    "    #https://www.sciencedirect.com/topics/medicine-and-dentistry/clinical-dementia-rating\n",
    "    if row['CDR'] == 0:\n",
    "      return \"Normal\"\n",
    "    if row['CDR'] == 0.5:\n",
    "      return \"Very Mild Dementia\"\n",
    "    if row['CDR'] == 1:\n",
    "      return \"Mild Dementia\"\n",
    "    if row['CDR'] == 2:\n",
    "      return \"Moderate Dementia\"\n",
    "    if row['CDR'] == 3:\n",
    "      return \"Severe Dementia\"\n",
    "    if row['CDR'] == 4:\n",
    "      return \"Severe Dementia\"\n",
    "    if row['CDR'] == 5:\n",
    "      return \"Severe Dementia\"\n",
    "    return \"Normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_hypothesis_determination(p_alpha, p_chi2, p_dof, pval, p_critical_value):\n",
    "    # Example: If alpha (significance) value is 0.05 or 5% it means 95% of confidence\n",
    "    confidence_perc = 1 - p_alpha\n",
    "    confidence_perc = 100 * confidence_perc\n",
    "    \n",
    "    p_alpha_perc = 100 * p_alpha\n",
    "    \n",
    "    str_H1 = \"I have enough evidence to reject H0 (There is a relationship between the categorical variables). Therefore, I assume H1 with a confidence of {0}%, significance of {1}%, statistic of {2}, degree of freedom of {3}, p_value of {4} and  crital value of {5}.\"\n",
    "    str_H0 = \"I don't have enough evidence to reject H0 (There is no relationship between 2 categorical variables). So we accept is true with a confidence of {0}%, significance of {1}%, statistic of {2}, degree of freedom of {3}, p_value of {4} and  crital value of {5}.\"\n",
    "    \n",
    "    print(\"Validation_1:\\n\")\n",
    "    if abs(chi2)>=critical_value:\n",
    "        print(\"Validating chi2>=critical_value: \" + str_H1.format(confidence_perc,p_alpha_perc, p_chi2, p_dof, p_value, p_critical_value))\n",
    "    else:\n",
    "        print(\"Validating chi2>=critical_value: \" + str_H0.format(confidence_perc,p_alpha_perc, p_chi2, p_dof, p_value, p_critical_value))\n",
    "\n",
    "    print(\"\\nValidation_2:\\n\")\n",
    "    if pval<=alpha:\n",
    "        print(\"Validating pval<=alpha: \" + str_H1.format(confidence_perc,p_alpha_perc, p_chi2, p_dof, p_value, p_critical_value))\n",
    "    else:\n",
    "        print(\"Validating pval<=alpha: \" + str_H0.format(confidence_perc,p_alpha_perc, p_chi2, p_dof, p_value, p_critical_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetColsExplanation():\n",
    "    # The explanation of the cols was taken from: https://www.kaggle.com/data/60603\n",
    "    extra_explanation = (\"Country --> Country\",\n",
    "                         \"Year --> Year\",\n",
    "                        \"Status --> Developed or Developing status\",\n",
    "                        \"Life expectancy --> Life Expectancy in age\",\n",
    "                        \"Adult Mortality --> Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)\",\n",
    "                        \"infant deaths --> Number of Infant Deaths per 1000 population\",\n",
    "                        \"Alcohol --> Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)\",\n",
    "                        \"percentage expenditure -- Expenditure on health as a percentage of Gross Domestic Product per capita(%)\",\n",
    "                        \"Hepatitis B --> Hepatitis B (HepB) immunization coverage among 1-year-olds (%)\",\n",
    "                        \"Measles --> Measles - number of reported cases per 1000 population\",\n",
    "                        \"BMI --> Average Body Mass Index of entire population\",\n",
    "                        \"under-five deaths --> Number of under-five deaths per 1000 population\",\n",
    "                        \"Polio --> Polio (Pol3) immunization coverage among 1-year-olds (%)\",\n",
    "                        \"Total expenditure --> General government expenditure on health as a percentage of total government expenditure (%)\",\n",
    "                        \"Diphtheria --> Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)\",\n",
    "                        \"HIV/AIDS --> Deaths per 1 000 live births HIV/AIDS (0-4 years)\",\n",
    "                        \"GDP --> Gross Domestic Product per capita (in USD)\",\n",
    "                        \"Population --> Population of the country\",\n",
    "                        \"thinness 1-19 years --> Prevalence of thinness among children and adolescents for Age 10 to 19 (% )\",\n",
    "                        \"thinness 5-9 years --> Prevalence of thinness among children for Age 5 to 9(%)\",\n",
    "                        \"Income composition of resources --> Human Development Index in terms of income composition of resources (index ranging from 0 to 1)\",\n",
    "                        \"Schooling --> Number of years of Schooling(years)\"\n",
    "                        )\n",
    "    return extra_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixing_col_nulls(df):\n",
    "    for label,content in df.items():\n",
    "        if pd.isnull(content).sum():\n",
    "            df[label] = content.fillna(content.median())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sklean_eval_metrics(Y_train, Y_train_pred, Y_test, Y_test_pred):\n",
    "    print(\"Training set performance\")\n",
    "    print(\"MAE:\", mean_absolute_error(Y_train, Y_train_pred))\n",
    "    print(\"MSE:\", mean_squared_error(Y_train, Y_train_pred))\n",
    "    print(\"RMSE:\", sqrt(mean_squared_error(Y_train, Y_train_pred)))\n",
    "    print(\"RMSLE:\", mean_squared_log_error(Y_train, Y_train_pred))\n",
    "    print(\"R2 Score:\", r2_score(Y_train, Y_train_pred))\n",
    "    \n",
    "    print(\"\\n\")    \n",
    "    print(\"Testing set performance\")\n",
    "    print(\"MAE:\", mean_absolute_error(Y_test, Y_test_pred))\n",
    "    print(\"MSE:\", mean_squared_error(Y_test, Y_test_pred))\n",
    "    print(\"RMSE:\", sqrt(mean_squared_error(Y_test, Y_test_pred)))\n",
    "    print(\"RMSLE:\", mean_squared_log_error(Y_test, Y_test_pred))\n",
    "    print(\"R2 Score:\", r2_score(Y_test, Y_test_pred))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
